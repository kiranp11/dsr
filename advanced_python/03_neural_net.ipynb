{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "Y_train = mnist.train.labels\n",
    "Y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c363cdb38>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0,:].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "\n",
      "\n",
      "[[0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_with_loops(y):\n",
    "    encoded = np.zeros((len(y), np.max(y)+1), dtype=\"uint8\")\n",
    "    for i in range(len(y)):\n",
    "        encoded[i, y[i]] = 1\n",
    "    assert len(y) == encoded.shape[0]    \n",
    "    return encoded\n",
    "\n",
    "\n",
    "print(one_hot_encode_with_loops(np.r_[5,6,7]))\n",
    "print('\\n')\n",
    "print(one_hot_encode_with_loops(np.r_[5,6,7,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "\n",
      "\n",
      "[[0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(y):\n",
    "    encoded = np.zeros((len(y), np.max(y)+1), dtype=\"uint8\")\n",
    "    encoded[np.arange(len(y)), y] = 1\n",
    "    assert len(y) == encoded.shape[0]    \n",
    "    return encoded\n",
    "\n",
    "\n",
    "print(one_hot_encode(np.r_[5,6,7]))\n",
    "print('\\n')\n",
    "print(one_hot_encode(np.r_[5,6,7,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(y):\n",
    "    y = np.argmax(y, axis=1)\n",
    "    assert(len(y) == y.shape[0])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cases:\n",
    "assert np.all(one_hot_decode(one_hot_encode_with_loops(Y_train)) == Y_train)\n",
    "assert np.all(one_hot_decode(one_hot_encode_with_loops(Y_test)) == Y_test)\n",
    "\n",
    "assert np.all(one_hot_decode(one_hot_encode(Y_train)) == Y_train)\n",
    "assert np.all(one_hot_decode(one_hot_encode(Y_test)) == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train2 = one_hot_encode(Y_train)\n",
    "Y_test2 = one_hot_encode(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "# add a column of 1s as the new first column in X_train and X_test as the bias term 'c' in y = mx + c\n",
    "X_train2 = np.insert(X_train, 0, 1, axis=1)\n",
    "print(X_train2.shape)\n",
    "X_test2 = np.insert(X_test, 0, 1, axis=1)\n",
    "print(X_test2.shape)\n",
    "\n",
    "assert(np.all(X_train2[:,0] == 1.0))\n",
    "assert(X_train2.shape[1] == X_train.shape[1]+1)\n",
    "\n",
    "assert(np.all(X_test2[:,0] == 1.0))\n",
    "assert(X_test2.shape[1] == X_test.shape[1]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential function to model probabilities in logistic regression\n",
    "\n",
    "1. exp has range (0, infinity), always positive. f(a,x)\n",
    "2. differentiable\n",
    "3. Fitting a model is about finding a's so that f(a,x) models probability p(y=1/x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12329188,  0.1299769 ,  0.14773021, ...,  0.07997212,\n",
       "         0.06872342,  0.08614006],\n",
       "       [ 0.07494209,  0.07616166,  0.08631483, ...,  0.11815651,\n",
       "         0.17038993,  0.07219773],\n",
       "       [ 0.09112277,  0.0892307 ,  0.06860778, ...,  0.09890484,\n",
       "         0.1386554 ,  0.09602535],\n",
       "       ..., \n",
       "       [ 0.05695001,  0.10238205,  0.13725405, ...,  0.12168749,\n",
       "         0.07848671,  0.11800405],\n",
       "       [ 0.10662881,  0.12403117,  0.09113541, ...,  0.11919223,\n",
       "         0.10740963,  0.07578485],\n",
       "       [ 0.08277003,  0.12739818,  0.07352216, ...,  0.09246812,\n",
       "         0.14150941,  0.14834198]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(Y):\n",
    "    Z = np.zeros(Y.shape)\n",
    "    \"\"\"\n",
    "    input: Y - real-valued matrix n*10\n",
    "    output:Z - (0, 1) valued matrix , shape n*10\n",
    "    \n",
    "    \"\"\"\n",
    "    Yexp = np.exp(Y)\n",
    "\n",
    "    #without reshaping np.sum(Yexp, axis=1) will be of dim (1,n) becaus of shape broadcasting-add 1 to the beggining to match the bigger dim in this case n * 10)\n",
    "    #ValueError: operands could not be broadcast together with shapes (324523,10) (324523,) \n",
    "    \n",
    "    Z = Yexp / np.sum(Yexp, axis=1).reshape(-1,1)\n",
    "    \n",
    "    assert(np.all(Z > 0.0))\n",
    "    assert(np.all(Z < 1.0))\n",
    "    assert np.all(np.abs(np.sum(Z, axis=1) - 1.0) < 1e-6)\n",
    "    return Z  \n",
    "\n",
    "softmax(np.random.rand(324523,10))\n",
    "softmax(np.random.rand(666,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single layer N Net\n",
    "np.random.seed(123)\n",
    "A = np.random.rand(785,10)\n",
    "Y_pred = softmax(X_train2 @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-layer : softmax(X_train2 @ A1)\n",
    "* 2-layer : softmax(relu(X_train2 @ A1) @ A2)\n",
    "* 3-layer : softmax(relu(relu(X_train2 @ A1) @ A2) @ A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y_pred, Y_true):\n",
    "    return np.mean(Y_pred == Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.093254545454545451"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(one_hot_decode(Y_pred), Y_train) # Random prediction accuracy ~ 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitness function - maximize\n",
    "\n",
    "## accuracy(decode(softmax(X_train@A)), Y_train)\n",
    "\n",
    "but accuracy function is not differentiable because it has jumps, hence we can minimize error function instead\n",
    "\n",
    "....Or minimize Error\n",
    "\n",
    "## Error(A) = cross_entropy(softmax(X_train@A), Y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(A, X_train2, Y_train2):\n",
    "    \"\"\"\n",
    "     A.shape == (785,10)\n",
    "     X_train2.shape == (n, 785)\n",
    "     Y_train2.shape == (n, 10), just 0s and 1s\n",
    "     result erro measure > 0\n",
    "    \"\"\"\n",
    "    Y_pred = softmax(X_train2@A) # Y_pred.shape == (n,10)\n",
    "    return -np.sum(Y_train2*np.log(Y_pred))/X_train2.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates gradient - vector of partial derivatives of cross_entropy function w.r.t A\n",
    "def cross_entropy_grad(A, X_train2, Y_train2):\n",
    "    Y_pred = softmax(X_train2@A)\n",
    "    return -(X_train2.T @ (Y_train2 - Y_pred))/X_train2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.860018181818\n"
     ]
    }
   ],
   "source": [
    "# find A: cross_entropy_grad(A, X_train2, Y_train2) == 0\n",
    "\n",
    "def gradient_decent(A, X_train2, Y_train2):\n",
    "    np.random.seed(123)\n",
    "    A = np.random.rand(785,10) # init point\n",
    "    eta = 0.5 # learning rate\n",
    "    maxiter = 100\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        A = A - eta * cross_entropy_grad(A, X_train2, Y_train2)\n",
    "    return A\n",
    "\n",
    "A = gradient_decent(A, X_train2, Y_train2)\n",
    "Y_pred = softmax(X_train2@A)\n",
    "\n",
    "print(accuracy(one_hot_decode(Y_pred), Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above gradient decent can be improved\n",
    "\n",
    "1. ADAM - Adaptive momentum methods - If the direction hasn't changed b/w iterations may be we can take bigger steps (momentum)\n",
    "2. Stochastic Gradient decent - use 1 row each iter\n",
    "3. Mini batch gradient decent - use a subset of rows each iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: cross_train=0.465891 acc_train=0.859 acc_test=0.869\n",
      "200: cross_train=0.394692 acc_train=0.883 acc_test=0.891\n",
      "300: cross_train=0.365812 acc_train=0.892 acc_test=0.900\n",
      "400: cross_train=0.347166 acc_train=0.900 acc_test=0.905\n",
      "500: cross_train=0.335314 acc_train=0.903 acc_test=0.908\n",
      "600: cross_train=0.327027 acc_train=0.904 acc_test=0.910\n",
      "700: cross_train=0.318747 acc_train=0.908 acc_test=0.912\n",
      "800: cross_train=0.314052 acc_train=0.909 acc_test=0.913\n",
      "900: cross_train=0.309564 acc_train=0.911 acc_test=0.915\n",
      "1000: cross_train=0.304580 acc_train=0.913 acc_test=0.914\n",
      "1100: cross_train=0.301569 acc_train=0.913 acc_test=0.915\n",
      "1200: cross_train=0.299407 acc_train=0.914 acc_test=0.916\n",
      "1300: cross_train=0.295121 acc_train=0.916 acc_test=0.918\n",
      "1400: cross_train=0.291634 acc_train=0.918 acc_test=0.918\n",
      "1500: cross_train=0.289378 acc_train=0.918 acc_test=0.918\n",
      "1600: cross_train=0.287632 acc_train=0.918 acc_test=0.917\n",
      "1700: cross_train=0.285878 acc_train=0.919 acc_test=0.918\n",
      "1800: cross_train=0.286160 acc_train=0.919 acc_test=0.916\n",
      "1900: cross_train=0.282597 acc_train=0.920 acc_test=0.918\n",
      "2000: cross_train=0.281028 acc_train=0.920 acc_test=0.918\n",
      "2100: cross_train=0.279750 acc_train=0.921 acc_test=0.920\n",
      "2200: cross_train=0.279770 acc_train=0.921 acc_test=0.920\n",
      "2300: cross_train=0.277531 acc_train=0.922 acc_test=0.919\n",
      "2400: cross_train=0.275859 acc_train=0.922 acc_test=0.920\n",
      "2500: cross_train=0.275739 acc_train=0.923 acc_test=0.920\n",
      "2600: cross_train=0.273831 acc_train=0.924 acc_test=0.921\n",
      "2700: cross_train=0.273367 acc_train=0.923 acc_test=0.920\n",
      "2800: cross_train=0.273968 acc_train=0.923 acc_test=0.919\n",
      "2900: cross_train=0.272559 acc_train=0.923 acc_test=0.920\n",
      "3000: cross_train=0.269737 acc_train=0.925 acc_test=0.922\n",
      "3100: cross_train=0.269595 acc_train=0.924 acc_test=0.922\n",
      "3200: cross_train=0.269369 acc_train=0.925 acc_test=0.921\n",
      "3300: cross_train=0.267195 acc_train=0.925 acc_test=0.921\n",
      "3400: cross_train=0.266565 acc_train=0.926 acc_test=0.920\n",
      "3500: cross_train=0.266733 acc_train=0.925 acc_test=0.921\n",
      "3600: cross_train=0.266099 acc_train=0.926 acc_test=0.920\n",
      "3700: cross_train=0.266360 acc_train=0.925 acc_test=0.921\n",
      "3800: cross_train=0.266354 acc_train=0.925 acc_test=0.920\n",
      "3900: cross_train=0.263743 acc_train=0.926 acc_test=0.923\n",
      "4000: cross_train=0.262918 acc_train=0.926 acc_test=0.921\n",
      "4100: cross_train=0.263342 acc_train=0.927 acc_test=0.920\n",
      "4200: cross_train=0.261907 acc_train=0.927 acc_test=0.922\n",
      "4300: cross_train=0.261130 acc_train=0.926 acc_test=0.922\n",
      "4400: cross_train=0.261368 acc_train=0.927 acc_test=0.923\n",
      "4500: cross_train=0.260757 acc_train=0.927 acc_test=0.922\n",
      "4600: cross_train=0.261120 acc_train=0.927 acc_test=0.923\n",
      "4700: cross_train=0.259755 acc_train=0.928 acc_test=0.922\n",
      "4800: cross_train=0.259732 acc_train=0.928 acc_test=0.923\n",
      "4900: cross_train=0.258755 acc_train=0.928 acc_test=0.923\n",
      "5000: cross_train=0.258630 acc_train=0.928 acc_test=0.922\n",
      "5100: cross_train=0.259109 acc_train=0.928 acc_test=0.923\n",
      "5200: cross_train=0.258536 acc_train=0.928 acc_test=0.923\n",
      "5300: cross_train=0.257763 acc_train=0.929 acc_test=0.923\n",
      "5400: cross_train=0.257319 acc_train=0.928 acc_test=0.923\n",
      "5500: cross_train=0.256993 acc_train=0.928 acc_test=0.922\n",
      "5600: cross_train=0.256512 acc_train=0.928 acc_test=0.923\n",
      "5700: cross_train=0.255983 acc_train=0.929 acc_test=0.924\n",
      "5800: cross_train=0.255382 acc_train=0.929 acc_test=0.924\n",
      "5900: cross_train=0.255414 acc_train=0.929 acc_test=0.924\n",
      "6000: cross_train=0.255703 acc_train=0.929 acc_test=0.923\n",
      "6100: cross_train=0.254965 acc_train=0.929 acc_test=0.922\n",
      "6200: cross_train=0.255214 acc_train=0.929 acc_test=0.923\n",
      "6300: cross_train=0.253757 acc_train=0.930 acc_test=0.924\n",
      "6400: cross_train=0.254021 acc_train=0.930 acc_test=0.924\n",
      "6500: cross_train=0.253547 acc_train=0.930 acc_test=0.923\n",
      "6600: cross_train=0.253451 acc_train=0.929 acc_test=0.923\n",
      "6700: cross_train=0.253572 acc_train=0.929 acc_test=0.923\n",
      "6800: cross_train=0.252336 acc_train=0.930 acc_test=0.925\n",
      "6900: cross_train=0.253204 acc_train=0.930 acc_test=0.924\n",
      "7000: cross_train=0.252341 acc_train=0.930 acc_test=0.926\n",
      "7100: cross_train=0.253243 acc_train=0.929 acc_test=0.923\n",
      "7200: cross_train=0.251566 acc_train=0.930 acc_test=0.925\n",
      "7300: cross_train=0.252182 acc_train=0.929 acc_test=0.922\n",
      "7400: cross_train=0.252175 acc_train=0.929 acc_test=0.923\n",
      "7500: cross_train=0.251674 acc_train=0.930 acc_test=0.926\n",
      "7600: cross_train=0.250420 acc_train=0.930 acc_test=0.926\n",
      "7700: cross_train=0.250355 acc_train=0.931 acc_test=0.926\n",
      "7800: cross_train=0.249913 acc_train=0.931 acc_test=0.925\n",
      "7900: cross_train=0.250240 acc_train=0.931 acc_test=0.925\n",
      "8000: cross_train=0.249630 acc_train=0.931 acc_test=0.925\n",
      "8100: cross_train=0.250518 acc_train=0.930 acc_test=0.924\n",
      "8200: cross_train=0.248883 acc_train=0.931 acc_test=0.925\n",
      "8300: cross_train=0.249942 acc_train=0.931 acc_test=0.926\n",
      "8400: cross_train=0.248873 acc_train=0.931 acc_test=0.926\n",
      "8500: cross_train=0.248672 acc_train=0.931 acc_test=0.925\n",
      "8600: cross_train=0.248473 acc_train=0.931 acc_test=0.924\n",
      "8700: cross_train=0.247977 acc_train=0.931 acc_test=0.925\n",
      "8800: cross_train=0.248150 acc_train=0.931 acc_test=0.925\n",
      "8900: cross_train=0.249255 acc_train=0.931 acc_test=0.923\n",
      "9000: cross_train=0.247703 acc_train=0.931 acc_test=0.926\n",
      "9100: cross_train=0.247432 acc_train=0.931 acc_test=0.926\n",
      "9200: cross_train=0.247376 acc_train=0.931 acc_test=0.927\n",
      "9300: cross_train=0.247479 acc_train=0.932 acc_test=0.927\n",
      "9400: cross_train=0.246703 acc_train=0.932 acc_test=0.925\n",
      "9500: cross_train=0.246487 acc_train=0.931 acc_test=0.925\n",
      "9600: cross_train=0.246811 acc_train=0.931 acc_test=0.924\n",
      "9700: cross_train=0.246889 acc_train=0.932 acc_test=0.925\n",
      "9800: cross_train=0.246227 acc_train=0.932 acc_test=0.926\n",
      "9900: cross_train=0.246886 acc_train=0.932 acc_test=0.926\n",
      "10000: cross_train=0.246576 acc_train=0.932 acc_test=0.925\n",
      "0.931545454545\n"
     ]
    }
   ],
   "source": [
    "# gradient decent with sub sample every iter\n",
    "\n",
    "def gradient_decent_improved(A, X_train2, Y_train2):\n",
    "    np.random.seed(123)\n",
    "    A = np.random.rand(785,10) # init point\n",
    "    eta = 0.5 # learning rate\n",
    "    maxiter = 10000\n",
    "    subset_size = X_train2.shape[0] // 100 # 1% of the training sample -> 100\n",
    "    \n",
    "    for i in range(maxiter):\n",
    "        #generate subset_size random indexes\n",
    "        indices = np.random.randint(0,X_train2.shape[0],subset_size)\n",
    "        x_subset = X_train2[indices,:]\n",
    "        y_subset = Y_train2[indices,:]\n",
    "        A = A - eta * cross_entropy_grad(A, x_subset, y_subset)\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print(\"%3d: cross_train=%8f acc_train=%.3f acc_test=%.3f\" %\n",
    "                 (\n",
    "                     i+1,\n",
    "                     cross_entropy(A, X_train2, Y_train2),\n",
    "                     accuracy(Y_train, one_hot_decode(softmax(X_train2@A))),\n",
    "                     accuracy(Y_test, one_hot_decode(softmax(X_test2@A)))\n",
    "                 ))\n",
    "    return A\n",
    "\n",
    "A = gradient_decent_improved(A, X_train2, Y_train2)\n",
    "Y_pred = softmax(X_train2@A)\n",
    "\n",
    "print(accuracy(one_hot_decode(Y_pred), Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[332 454 922 428 345 402 762 113 339 624]\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.randint(0,1000,10)\n",
    "print(indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
